# BigDataA1

## Overview
This assignment is designed to support your in-class understanding of how big data analytics stacks work and get some hands-on experience in using them. You will need to deploy Apache Hadoop Distributed File System (HDFS) as the underlying file system and Apache Spark as the execution engine. You will then develop several small applications based on them. You will produce a short report detailing your observations and takeaways.


## Learning Outcomes
- The objectives of this assignment are to:

- Configure and deploy Apache HDFS and Apache Spark using remote SSH and Linux terminal.
- Write simple Spark applications and launch them in a cluster.
- Describe how Apache HDFS and Apache Spark work, and interact with each other.
